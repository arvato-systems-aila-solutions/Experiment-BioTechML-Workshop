{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a0574ef-b559-410d-a849-1f1e1440d432",
   "metadata": {},
   "source": [
    "# üñºÔ∏è Image Classification with BreastMNIST\n",
    "## A Practical Guide to Medical Image ML (90 minutes)\n",
    "\n",
    "---\n",
    "\n",
    "### üìã Workshop Overview\n",
    "\n",
    "Welcome! In this tutorial, you'll learn how to build **machine learning models for medical image classification**. We'll work with the BreastMNIST dataset and compare traditional ML methods with deep learning approaches.\n",
    "\n",
    "### üéØ What You'll Learn\n",
    "\n",
    "1. **Data Exploration** - Understanding medical image data\n",
    "2. **Model Training** - Building and comparing multiple models\n",
    "3. **Model Evaluation** - Analyzing performance metrics\n",
    "\n",
    "---\n",
    "\n",
    "### üìä About Our Dataset\n",
    "\n",
    "We're working with **BreastMNIST** - ultrasound images of breast tissue:\n",
    "- 28x28 grayscale images\n",
    "- Binary classification: Benign (0) vs Malignant (1)\n",
    "- Real medical imaging data\n",
    "- For more information, visit https://medmnist.com/\n",
    "\n",
    "**Goal**: Classify breast ultrasound images to aid in cancer diagnosis.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è Prerequisites Check\n",
    "\n",
    "First, we need to install the required library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0c7cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install medmnist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70819e8",
   "metadata": {},
   "source": [
    "Let's verify all required libraries are installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672b47ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Check required libraries and their versions\n",
    "required_libs = {\n",
    "    \"medmnist\": \"3.0\",\n",
    "    \"numpy\": \"1.24\",\n",
    "    \"scikit-learn\": \"1.3\",\n",
    "    \"matplotlib\": \"3.7\",\n",
    "    \"seaborn\": \"0.12\",\n",
    "    \"tensorflow\": \"2.13\",\n",
    "    \"scipy\": \"1.10\"\n",
    "}\n",
    "\n",
    "print(\"üîç Checking installed libraries...\\n\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for lib_name, min_version in required_libs.items():\n",
    "    try:\n",
    "        if lib_name == \"scikit-learn\":\n",
    "            import sklearn\n",
    "            lib = sklearn\n",
    "            actual_name = \"sklearn\"\n",
    "        else:\n",
    "            lib = __import__(lib_name)\n",
    "            actual_name = lib_name\n",
    "        \n",
    "        installed_version = lib.__version__\n",
    "        print(f\"‚úì {lib_name}: {installed_version} (required: >={min_version})\")\n",
    "    except ImportError:\n",
    "        print(f\"‚úó {lib_name} is NOT installed. Please install it using:\")\n",
    "        print(f\"   pip install {lib_name}>={min_version}\")\n",
    "\n",
    "print(\"\\n‚úÖ Library check complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e179b11-81a4-46e9-80d8-97501960ab6d",
   "metadata": {},
   "source": [
    "## üîç STEP 1: Data Exploration\n",
    "\n",
    "Let's understand our medical image dataset before diving into modeling!\n",
    "\n",
    "### üì• Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851630a3-7590-45e0-81f0-42123f57cf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from medmnist import BreastMNIST\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"üì• Loading BreastMNIST dataset...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Load dataset\n",
    "train_ds = BreastMNIST(split='train', download=True)\n",
    "val_ds = BreastMNIST(split='val', download=True)\n",
    "test_ds = BreastMNIST(split='test', download=True)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X_train = train_ds.imgs.reshape(len(train_ds), -1)\n",
    "y_train = train_ds.labels.flatten()\n",
    "\n",
    "X_val = val_ds.imgs.reshape(len(val_ds), -1)\n",
    "y_val = val_ds.labels.flatten()\n",
    "\n",
    "X_test = test_ds.imgs.reshape(len(test_ds), -1)\n",
    "y_test = test_ds.labels.flatten()\n",
    "\n",
    "print(f\"\\n‚úÖ Dataset loaded successfully!\")\n",
    "print(f\"\\nüìä Dataset Overview:\")\n",
    "print(f\"   Training samples: {len(X_train)}\")\n",
    "print(f\"   Validation samples: {len(X_val)}\")\n",
    "print(f\"   Test samples: {len(X_test)}\")\n",
    "print(f\"   Image dimensions: 28x28 pixels\")\n",
    "print(f\"   Flattened features: {X_train.shape[1]} pixels\")\n",
    "print(f\"   Classes: {np.unique(y_test)} --> 2 (0=Benign, 1=Malignant)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd54824-916e-4548-8b8e-e149425ceea6",
   "metadata": {},
   "source": [
    "### üîÑ Data Preparation for Traditional ML\n",
    "\n",
    "For traditional ML models (Logistic Regression, Random Forest, etc.), we need to:\n",
    "1. Flatten images from 28x28 to 784 features\n",
    "2. Normalize pixel values using StandardScaler\n",
    "\n",
    "Attention: Make sure to fit the scaler only on the training data to avoid data leakage!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3f5294-531f-4cd6-8e5e-44290db1d8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)  \n",
    "X_val   = scaler.transform(X_val)\n",
    "X_test  = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517769b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüî¢ Data Shape After Normalization:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"   Training set:   {X_train.shape[0]:>5} samples √ó {X_train.shape[1]:>3} features\")\n",
    "print(f\"   Validation set: {X_val.shape[0]:>5} samples √ó {X_val.shape[1]:>3} features\")\n",
    "print(f\"   Test set:       {X_test.shape[0]:>5} samples √ó {X_test.shape[1]:>3} features\")\n",
    "print(f\"\\n‚úÖ Data normalized and ready for traditional ML models!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9feb4738-4f18-4b66-b68f-9d8ebe233338",
   "metadata": {},
   "source": [
    "### üìä Class Distribution Analysis\n",
    "\n",
    "Let's check if our dataset is balanced between benign and malignant cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a81766-fba3-482f-aa3b-8e92df1871d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìä Analyzing class distribution...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "# Separate data by class for individual coloring\n",
    "benign_data = y_train[y_train == 0]\n",
    "malignant_data = y_train[y_train == 1]\n",
    "\n",
    "# Plot each class separately with different colors\n",
    "plt.hist([benign_data, malignant_data], bins=2, edgecolor=\"black\", \n",
    "         color=['#2ecc71', '#e74c3c'], label=['Benign', 'Malignant'])\n",
    "\n",
    "plt.xticks([0,1], ['Benign', 'Malignant'])\n",
    "plt.title(\"Class Distribution in Training Set\", fontsize=14, fontweight='bold')\n",
    "plt.xlabel(\"Label\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "benign_count = (y_train==0).sum()\n",
    "malignant_count = (y_train==1).sum()\n",
    "total = len(y_train)\n",
    "\n",
    "print(f\"\\nüìà Class Distribution:\")\n",
    "print(f\"   Benign (0):    {benign_count:>4} samples ({benign_count/total*100:.1f}%)\")\n",
    "print(f\"   Malignant (1): {malignant_count:>4} samples ({malignant_count/total*100:.1f}%)\")\n",
    "print(f\"   Total:         {total:>4} samples\")\n",
    "\n",
    "if abs(benign_count - malignant_count) / total > 0.2:\n",
    "    print(\"\\n‚ö†Ô∏è  Dataset is imbalanced! Consider using stratified sampling or class weights.\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ Dataset is reasonably balanced.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80c6242-1847-462d-bb7d-1602908c0264",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "print(\"\\nüñºÔ∏è  Sample Images from Training Set:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Note: X_train is normalized, we need to use original images\n",
    "X_train_original = train_ds.imgs\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i in range(12):\n",
    "    idx = random.randint(0, len(X_train_original)-1)\n",
    "    plt.subplot(2, 6, i+1)\n",
    "    plt.imshow(X_train_original[idx], cmap=\"gray\")\n",
    "    label_text = \"Benign\" if y_train[idx] == 0 else \"Malignant\"\n",
    "    plt.title(f\"{label_text}\", fontsize=9)\n",
    "    plt.axis(\"off\")\n",
    "plt.suptitle(\"Random Sample Images\", fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7b502b-61a0-4399-a569-9ef7e7ba682f",
   "metadata": {},
   "source": [
    "### üé® Dimensionality Visualization with PCA\n",
    "\n",
    "Let's use PCA to visualize how the two classes separate in 2D space when using raw pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046442b2-9b8b-45f9-93bd-a397989cdb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "print(\"\\nüé® Performing PCA for 2D visualization...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "X_flat = X_train.reshape(len(X_train), -1)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_flat)\n",
    "\n",
    "print(f\"   Explained variance: {pca.explained_variance_ratio_.sum()*100:.2f}%\")\n",
    "print(f\"   PC1: {pca.explained_variance_ratio_[0]*100:.2f}%\")\n",
    "print(f\"   PC2: {pca.explained_variance_ratio_[1]*100:.2f}%\")\n",
    "\n",
    "plt.figure(figsize=(8, 7))\n",
    "scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y_train, cmap=\"coolwarm\", s=3, alpha=0.6)\n",
    "plt.title(\"PCA of BreastMNIST (2D Projection)\", fontsize=14, fontweight='bold')\n",
    "plt.xlabel(f\"PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)\")\n",
    "plt.ylabel(f\"PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)\")\n",
    "plt.colorbar(scatter, label=\"Label (0=Benign, 1=Malignant)\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ PCA visualization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c47bc2-d4c4-4e8f-ab6d-20d106cc7d8c",
   "metadata": {},
   "source": [
    "### üîç Image Properties Analysis\n",
    "\n",
    "**Key Challenge with these Medical Images:** Tumors can appear at different locations in ultrasound images!\n",
    "\n",
    "‚ùå **Problem:** Raw pixel values are **location-dependent** - if a tumor appears on the left vs right side, pixel values at specific positions will be completely different.\n",
    "\n",
    "‚úÖ **Solution approach:** Extract **location-independent features** like texture, contrast, and overall brightness that describe the image regardless of where features appear spatially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a718f6c8-1610-4ec6-b243-7c9ec57de87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import entropy\n",
    "import numpy as np\n",
    "\n",
    "print(\"\\nüìä Computing basic image statistics (location-independent)...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Use original unnormalized images for meaningful statistics\n",
    "X_train_original = train_ds.imgs\n",
    "\n",
    "def compute_image_features(images):\n",
    "    \"\"\"Compute location-independent image features\"\"\"\n",
    "    features = {\n",
    "        'mean_intensity': [],\n",
    "        'std_intensity': [],\n",
    "        'contrast': [],\n",
    "        'entropy': [],\n",
    "        'brightness': []\n",
    "    }\n",
    "    \n",
    "    for img in images:\n",
    "        # Overall brightness and variability\n",
    "        features['mean_intensity'].append(img.mean())\n",
    "        features['std_intensity'].append(img.std())\n",
    "        \n",
    "        # Contrast (range of intensities)\n",
    "        features['contrast'].append(img.max() - img.min())\n",
    "        \n",
    "        # Entropy (texture complexity)\n",
    "        hist, _ = np.histogram(img.flatten(), bins=50, range=(0, 255))\n",
    "        hist = hist / hist.sum()\n",
    "        features['entropy'].append(entropy(hist + 1e-10))  # Add small value to avoid log(0)\n",
    "        \n",
    "        # Brightness percentiles\n",
    "        features['brightness'].append(np.percentile(img, 75))  # 75th percentile\n",
    "    \n",
    "    return {k: np.array(v) for k, v in features.items()}\n",
    "\n",
    "benign_features = compute_image_features(X_train_original[y_train == 0])\n",
    "malignant_features = compute_image_features(X_train_original[y_train == 1])\n",
    "\n",
    "print(f\"\\nüìà Image Feature Statistics:\")\n",
    "print(f\"\\n   Mean Intensity (overall brightness):\")\n",
    "print(f\"      Benign:    {benign_features['mean_intensity'].mean():.2f} ¬± {benign_features['mean_intensity'].std():.2f}\")\n",
    "print(f\"      Malignant: {malignant_features['mean_intensity'].mean():.2f} ¬± {malignant_features['mean_intensity'].std():.2f}\")\n",
    "\n",
    "print(f\"\\n   Standard Deviation (local variation):\")\n",
    "print(f\"      Benign:    {benign_features['std_intensity'].mean():.2f} ¬± {benign_features['std_intensity'].std():.2f}\")\n",
    "print(f\"      Malignant: {malignant_features['std_intensity'].mean():.2f} ¬± {malignant_features['std_intensity'].std():.2f}\")\n",
    "\n",
    "print(f\"\\n   Contrast (max-min intensity range):\")\n",
    "print(f\"      Benign:    {benign_features['contrast'].mean():.2f} ¬± {benign_features['contrast'].std():.2f}\")\n",
    "print(f\"      Malignant: {malignant_features['contrast'].mean():.2f} ¬± {malignant_features['contrast'].std():.2f}\")\n",
    "\n",
    "print(f\"\\n   Entropy (texture complexity):\")\n",
    "print(f\"      Benign:    {benign_features['entropy'].mean():.3f} ¬± {benign_features['entropy'].std():.3f}\")\n",
    "print(f\"      Malignant: {malignant_features['entropy'].mean():.3f} ¬± {malignant_features['entropy'].std():.3f}\")\n",
    "\n",
    "print(\"\\nüí° These features work regardless of tumor location!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4e6b32-f423-463e-97af-0b88ef9a54dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"\\nüìä Visualizing feature distributions...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Contrast comparison\n",
    "axes[0, 0].hist(benign_features['contrast'], bins=30, alpha=0.6, label=\"Benign\", color='#2ecc71', density=True)\n",
    "axes[0, 0].hist(malignant_features['contrast'], bins=30, alpha=0.6, label=\"Malignant\", color='#e74c3c', density=True)\n",
    "axes[0, 0].set_xlabel(\"Contrast (intensity range)\", fontsize=10)\n",
    "axes[0, 0].set_ylabel(\"Density\", fontsize=10)\n",
    "axes[0, 0].set_title(\"Image Contrast Distribution\", fontweight='bold')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "# Entropy comparison\n",
    "axes[0, 1].hist(benign_features['entropy'], bins=30, alpha=0.6, label=\"Benign\", color='#2ecc71', density=True)\n",
    "axes[0, 1].hist(malignant_features['entropy'], bins=30, alpha=0.6, label=\"Malignant\", color='#e74c3c', density=True)\n",
    "axes[0, 1].set_xlabel(\"Entropy (texture complexity)\", fontsize=10)\n",
    "axes[0, 1].set_ylabel(\"Density\", fontsize=10)\n",
    "axes[0, 1].set_title(\"Texture Entropy Distribution\", fontweight='bold')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "# Std deviation comparison\n",
    "axes[1, 0].hist(benign_features['std_intensity'], bins=30, alpha=0.6, label=\"Benign\", color='#2ecc71', density=True)\n",
    "axes[1, 0].hist(malignant_features['std_intensity'], bins=30, alpha=0.6, label=\"Malignant\", color='#e74c3c', density=True)\n",
    "axes[1, 0].set_xlabel(\"Standard Deviation\", fontsize=10)\n",
    "axes[1, 0].set_ylabel(\"Density\", fontsize=10)\n",
    "axes[1, 0].set_title(\"Pixel Intensity Variability\", fontweight='bold')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(alpha=0.3)\n",
    "\n",
    "# Mean intensity comparison\n",
    "axes[1, 1].hist(benign_features['mean_intensity'], bins=30, alpha=0.6, label=\"Benign\", color='#2ecc71', density=True)\n",
    "axes[1, 1].hist(malignant_features['mean_intensity'], bins=30, alpha=0.6, label=\"Malignant\", color='#e74c3c', density=True)\n",
    "axes[1, 1].set_xlabel(\"Mean Intensity\", fontsize=10)\n",
    "axes[1, 1].set_ylabel(\"Density\", fontsize=10)\n",
    "axes[1, 1].set_title(\"Overall Brightness Distribution\", fontweight='bold')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Feature visualization complete!\")\n",
    "print(\"üí° These distributions help identify which features might be useful for classification!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1453d5e8",
   "metadata": {},
   "source": [
    "### üìê Edge Detection Analysis\n",
    "\n",
    "Medical images often contain important edge information (tumor boundaries, tissue structures).\n",
    "Let's analyze edge content using Sobel filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfad82b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import ndimage\n",
    "import numpy as np\n",
    "\n",
    "print(\"\\nüìê Analyzing edge content using Sobel filters...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "def compute_edge_features(images, n_samples=200):\n",
    "    \"\"\"Compute edge strength features\"\"\"\n",
    "    edge_strengths = []\n",
    "    \n",
    "    for img in images[:n_samples]:  # Limit for performance\n",
    "        # Sobel filters for horizontal and vertical edges\n",
    "        sobel_x = ndimage.sobel(img, axis=0)\n",
    "        sobel_y = ndimage.sobel(img, axis=1)\n",
    "        \n",
    "        # Edge magnitude\n",
    "        edge_magnitude = np.sqrt(sobel_x**2 + sobel_y**2)\n",
    "        \n",
    "        # Average edge strength (location-independent)\n",
    "        edge_strengths.append(edge_magnitude.mean())\n",
    "    \n",
    "    return np.array(edge_strengths)\n",
    "\n",
    "benign_edges = compute_edge_features(X_train_original[y_train == 0])\n",
    "malignant_edges = compute_edge_features(X_train_original[y_train == 1])\n",
    "\n",
    "print(f\"\\nüìä Edge Strength Statistics:\")\n",
    "print(f\"   Benign:    {benign_edges.mean():.2f} ¬± {benign_edges.std():.2f}\")\n",
    "print(f\"   Malignant: {malignant_edges.mean():.2f} ¬± {malignant_edges.std():.2f}\")\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(benign_edges, bins=30, alpha=0.6, label=\"Benign\", color='#2ecc71', density=True)\n",
    "plt.hist(malignant_edges, bins=30, alpha=0.6, label=\"Malignant\", color='#e74c3c', density=True)\n",
    "plt.xlabel(\"Average Edge Strength\", fontsize=12)\n",
    "plt.ylabel(\"Density\", fontsize=12)\n",
    "plt.title(\"Edge Content Analysis (Sobel Filter)\", fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Edge strength measures structural complexity independent of location!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824a6834",
   "metadata": {},
   "source": [
    "### üìä Visualize Sample Images with Edge Detection\n",
    "\n",
    "Let's see how edge detection highlights structural features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e740d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüì∏ Visualizing sample images with edge detection...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Select sample images\n",
    "benign_idx = np.where(y_train == 0)[0][0]\n",
    "malignant_idx = np.where(y_train == 1)[0][0]\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(12, 8))\n",
    "\n",
    "for i, (idx, label) in enumerate([(benign_idx, \"Benign\"), (malignant_idx, \"Malignant\")]):\n",
    "    img = X_train_original[idx]\n",
    "    \n",
    "    # Original image\n",
    "    axes[i, 0].imshow(img, cmap='gray')\n",
    "    axes[i, 0].set_title(f\"{label} - Original\", fontweight='bold')\n",
    "    axes[i, 0].axis('off')\n",
    "    \n",
    "    # Sobel X (vertical edges)\n",
    "    sobel_x = ndimage.sobel(img, axis=0)\n",
    "    axes[i, 1].imshow(sobel_x, cmap='gray')\n",
    "    axes[i, 1].set_title(\"Vertical Edges\", fontweight='bold')\n",
    "    axes[i, 1].axis('off')\n",
    "    \n",
    "    # Sobel Y (horizontal edges)\n",
    "    sobel_y = ndimage.sobel(img, axis=1)\n",
    "    axes[i, 2].imshow(sobel_y, cmap='gray')\n",
    "    axes[i, 2].set_title(\"Horizontal Edges\", fontweight='bold')\n",
    "    axes[i, 2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Edge detection visualization complete!\")\n",
    "print(\"üí° Edges highlight structural boundaries regardless of their location!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2d408a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üî¨ CRITICAL INSIGHT: Why Raw Pixels Don't Work Well\n",
    "\n",
    "Before we start training models, let's understand a fundamental problem:\n",
    "\n",
    "### ‚ùå The Problem with Pixel-Based Approaches\n",
    "\n",
    "When we flatten images to 784 features (28√ó28 pixels), we're telling the model:\n",
    "- \"Feature 1\" = pixel at position (0,0)  \n",
    "- \"Feature 2\" = pixel at position (0,1)\n",
    "- ... and so on\n",
    "\n",
    "**This creates a huge problem for medical images:**\n",
    "\n",
    "If a tumor appears at different locations, the \"important pixels\" change completely! A tumor on the left side has high intensity at different pixel positions than a tumor on the right side.\n",
    "\n",
    "### ‚úÖ The Solution: Feature Engineering\n",
    "\n",
    "Instead of using raw pixel positions, we'll extract **location-independent features**:\n",
    "- **Contrast**: Difference between brightest and darkest areas\n",
    "- **Entropy**: Texture complexity (regardless of where texture appears)\n",
    "- **Edge Strength**: How pronounced boundaries are (not where they are)\n",
    "- **Statistical Moments**: Mean, standard deviation, percentiles\n",
    "\n",
    "Let's build a feature-based training set and compare it to the pixel-based approach!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d869bea",
   "metadata": {},
   "source": [
    "### üõ†Ô∏è Building a Feature-Based Dataset\n",
    "\n",
    "Now let's extract meaningful features from all images in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4266ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "from scipy import ndimage\n",
    "import pandas as pd\n",
    "\n",
    "# New:\n",
    "from skimage.feature import graycomatrix, graycoprops\n",
    "from skimage.filters import sobel\n",
    "\n",
    "print(\"\\nüîß Extracting location-independent features from all images...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "def extract_all_features(images, hist_bins=16, glcm_levels=32):\n",
    "    \"\"\"\n",
    "    Extract a comprehensive set of location-independent features from images.\n",
    "    These features describe WHAT is in the image, not WHERE it is.\n",
    "    \"\"\"\n",
    "    features_list = []\n",
    "    \n",
    "    for img in images:\n",
    "        # Ensure numpy array & float format\n",
    "        img = np.array(img).astype(np.float32)\n",
    "        \n",
    "        # Normalize to 0‚Äì1 if image is in 0‚Äì255 range\n",
    "        if img.max() > 1.0:\n",
    "            img_norm = img / 255.0\n",
    "        else:\n",
    "            img_norm = img.copy()\n",
    "        \n",
    "        feature_dict = {}\n",
    "        \n",
    "        # ===== Basic Statistics =====\n",
    "        feature_dict['mean_intensity'] = img.mean()\n",
    "        feature_dict['std_intensity'] = img.std()\n",
    "        feature_dict['max_intensity'] = img.max()\n",
    "        feature_dict['min_intensity'] = img.min()\n",
    "        feature_dict['median_intensity'] = np.median(img)\n",
    "        \n",
    "        # ===== Contrast & Range =====\n",
    "        feature_dict['contrast'] = img.max() - img.min()\n",
    "        feature_dict['intensity_range'] = np.ptp(img)  # Peak-to-peak difference\n",
    "        \n",
    "        # ===== Distribution Statistics =====\n",
    "        feature_dict['q25'] = np.percentile(img, 25)\n",
    "        feature_dict['q75'] = np.percentile(img, 75)\n",
    "        feature_dict['iqr'] = feature_dict['q75'] - feature_dict['q25']\n",
    "        feature_dict['skewness'] = ((img - img.mean()) ** 3).mean() / (img.std() ** 3 + 1e-10)\n",
    "        feature_dict['kurtosis'] = ((img - img.mean()) ** 4).mean() / (img.std() ** 4 + 1e-10)\n",
    "        \n",
    "        # ===== Intensity Proportions (Threshold-based) =====\n",
    "        # Relative fractions of dark / mid / bright pixels (helpful for ultrasound texture)\n",
    "        feature_dict['frac_low']  = np.mean(img_norm < 0.3)\n",
    "        feature_dict['frac_mid']  = np.mean((img_norm >= 0.3) & (img_norm <= 0.7))\n",
    "        feature_dict['frac_high'] = np.mean(img_norm > 0.7)\n",
    "        \n",
    "        # ===== Histogram & Entropy (Texture Complexity) =====\n",
    "        hist, bin_edges = np.histogram(img.flatten(), bins=hist_bins, range=(0, 255))\n",
    "        hist = hist.astype(np.float32)\n",
    "        hist = hist / (hist.sum() + 1e-10)\n",
    "        \n",
    "        # Histogram bins\n",
    "        for i in range(hist_bins):\n",
    "            feature_dict[f'hist_bin_{i}'] = hist[i]\n",
    "        \n",
    "        # Shannon entropy\n",
    "        feature_dict['entropy'] = entropy(hist + 1e-10)\n",
    "        \n",
    "        # ===== Edge / Gradient Features (Sobel) =====\n",
    "        sobel_x = ndimage.sobel(img_norm, axis=0)\n",
    "        sobel_y = ndimage.sobel(img_norm, axis=1)\n",
    "        edge_magnitude = np.sqrt(sobel_x**2 + sobel_y**2)\n",
    "        \n",
    "        feature_dict['edge_mean'] = edge_magnitude.mean()\n",
    "        feature_dict['edge_std'] = edge_magnitude.std()\n",
    "        feature_dict['edge_max'] = edge_magnitude.max()\n",
    "        feature_dict['edge_energy'] = (edge_magnitude ** 2).mean()\n",
    "        \n",
    "        # Fraction of \"strong\" edges\n",
    "        thr = np.percentile(edge_magnitude, 90)\n",
    "        feature_dict['edge_strong_frac'] = np.mean(edge_magnitude > thr)\n",
    "        \n",
    "        # ===== Laplacian (Sharpness / Fine Texture) =====\n",
    "        lap = ndimage.laplace(img_norm)\n",
    "        feature_dict['laplacian_mean'] = lap.mean()\n",
    "        feature_dict['laplacian_std'] = lap.std()\n",
    "        feature_dict['laplacian_energy'] = (lap ** 2).mean()\n",
    "        \n",
    "        # ===== GLCM Texture Features (Haralick-style, global) =====\n",
    "        # Quantize image for GLCM\n",
    "        img_q = img_norm.copy()\n",
    "        img_q = np.clip(img_q, 0, 1)\n",
    "        img_q = (img_q * (glcm_levels - 1)).astype(np.uint8)\n",
    "        \n",
    "        distances = [1]\n",
    "        angles = [0, np.pi/4, np.pi/2, 3*np.pi/4]  # 0¬∞, 45¬∞, 90¬∞, 135¬∞\n",
    "        \n",
    "        glcm = graycomatrix(\n",
    "            img_q,\n",
    "            distances=distances,\n",
    "            angles=angles,\n",
    "            levels=glcm_levels,\n",
    "            symmetric=True,\n",
    "            normed=True\n",
    "        )\n",
    "        \n",
    "        glcm_props = ['contrast', 'energy', 'homogeneity', 'correlation', 'dissimilarity']\n",
    "        for prop in glcm_props:\n",
    "            vals = graycoprops(glcm, prop)  # (len(distances), len(angles))\n",
    "            feature_dict[f'glcm_{prop}_mean'] = vals.mean()\n",
    "            feature_dict[f'glcm_{prop}_std'] = vals.std()\n",
    "        \n",
    "        # ===== Simple Frequency / DCT Features =====\n",
    "        # 2D Fourier magnitude with light Gaussian smoothing\n",
    "        dct_x = ndimage.fourier.fourier_gaussian(np.fft.fft2(img_norm), sigma=0.5)\n",
    "        dct_mag = np.abs(dct_x)\n",
    "        \n",
    "        # Low- vs. total frequency energy\n",
    "        h, w = dct_mag.shape\n",
    "        center_h, center_w = h // 2, w // 2\n",
    "        \n",
    "        # Small centered block = low-frequency area\n",
    "        low_block = dct_mag[center_h-3:center_h+3, center_w-3:center_w+3]\n",
    "        feature_dict['dct_low_energy'] = (low_block ** 2).mean()\n",
    "        feature_dict['dct_total_energy'] = (dct_mag ** 2).mean()\n",
    "        feature_dict['dct_low_ratio'] = feature_dict['dct_low_energy'] / (feature_dict['dct_total_energy'] + 1e-10)\n",
    "        \n",
    "        features_list.append(feature_dict)\n",
    "    \n",
    "    return pd.DataFrame(features_list)\n",
    "\n",
    "# Extract features for all datasets\n",
    "print(\"\\n   Processing training set...\")\n",
    "X_train_features = extract_all_features(train_ds.imgs)\n",
    "\n",
    "print(\"   Processing validation set...\")\n",
    "X_val_features = extract_all_features(val_ds.imgs)\n",
    "\n",
    "print(\"   Processing test set...\")\n",
    "X_test_features = extract_all_features(test_ds.imgs)\n",
    "\n",
    "print(f\"\\n‚úÖ Feature extraction complete!\")\n",
    "print(f\"\\nüìä Feature Set Overview:\")\n",
    "print(f\"   Number of features: {X_train_features.shape[1]}\")\n",
    "print(f\"   Training samples: {X_train_features.shape[0]}\")\n",
    "print(f\"   Validation samples: {X_val_features.shape[0]}\")\n",
    "print(f\"   Test samples: {X_test_features.shape[0]}\")\n",
    "\n",
    "print(f\"\\nüìã Extracted Features:\")\n",
    "for i, col in enumerate(X_train_features.columns, 1):\n",
    "    print(f\"   {i:2}. {col}\")\n",
    "\n",
    "print(f\"\\nüìä Sample Feature Values (first 3 training images):\")\n",
    "display(X_train_features.head(3).round(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a3cc40",
   "metadata": {},
   "source": [
    "### üîÑ Normalize Features\n",
    "\n",
    "Just like with pixel data, we need to normalize our features for optimal model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cad40aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "print(\"\\nüîÑ Normalizing features...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create and fit scaler on training data\n",
    "scaler_features = StandardScaler()\n",
    "X_train_feat_scaled = scaler_features.fit_transform(X_train_features)\n",
    "X_val_feat_scaled = scaler_features.transform(X_val_features)\n",
    "X_test_feat_scaled = scaler_features.transform(X_test_features)\n",
    "\n",
    "print(f\"\\n‚úÖ Features normalized!\")\n",
    "print(f\"\\nüìä Comparison:\")\n",
    "print(f\"   Pixel-based approach:   {X_train.shape[1]} features (784 pixels)\")\n",
    "print(f\"   Feature-based approach: {X_train_feat_scaled.shape[1]} features (handcrafted)\")\n",
    "print(f\"\\nüí° We reduced dimensionality by {X_train.shape[1] / X_train_feat_scaled.shape[1]:.1f}x while keeping meaningful information!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7242061-13e4-482a-94dd-6b53e6f46cc6",
   "metadata": {},
   "source": [
    "## ü§ñ STEP 2: Model Training - Comparing Approaches\n",
    "\n",
    "Now we'll train models using **BOTH approaches** to see which works better:\n",
    "\n",
    "1. **Pixel-Based**: Raw 784 pixel values (location-dependent)\n",
    "2. **Feature-Based**: 19 handcrafted features (location-independent)\n",
    "\n",
    "Let's see which approach wins! üèÜ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af089c3c-728c-4b51-b2d0-e1f495a7156c",
   "metadata": {},
   "source": [
    "### üìà Model 1: Logistic Regression\n",
    "\n",
    "A simple linear classifier - our baseline model. We'll train it with BOTH approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e02d9cc-66a7-4ed6-9016-7e9ff120a8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import time\n",
    "\n",
    "print(\"\\nüîÑ Training Logistic Regression with BOTH approaches...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ===== APPROACH 1: Pixel-Based =====\n",
    "print(\"\\n1Ô∏è‚É£ Pixel-Based Approach (784 features)\")\n",
    "start = time.time()\n",
    "logreg_pixels = LogisticRegression(max_iter=200, random_state=42)\n",
    "logreg_pixels.fit(X_train, y_train)\n",
    "train_time_pixels = time.time() - start\n",
    "\n",
    "y_pred_pixels = logreg_pixels.predict(X_test)\n",
    "\n",
    "logreg_pixels_results = {\n",
    "    \"accuracy\":  accuracy_score(y_test, y_pred_pixels),\n",
    "    \"precision\": precision_score(y_test, y_pred_pixels),\n",
    "    \"recall\":    recall_score(y_test, y_pred_pixels),\n",
    "    \"f1_score\":  f1_score(y_test, y_pred_pixels),\n",
    "    \"train_time_sec\": train_time_pixels\n",
    "}\n",
    "\n",
    "print(f\"   ‚úì Trained in {train_time_pixels:.2f}s | Accuracy: {logreg_pixels_results['accuracy']:.4f} | F1: {logreg_pixels_results['f1_score']:.4f}\")\n",
    "\n",
    "# ===== APPROACH 2: Feature-Based =====\n",
    "print(\"\\n2Ô∏è‚É£ Feature-Based Approach (19 features)\")\n",
    "start = time.time()\n",
    "logreg_features = LogisticRegression(max_iter=200, random_state=42)\n",
    "logreg_features.fit(X_train_feat_scaled, y_train)\n",
    "train_time_features = time.time() - start\n",
    "\n",
    "y_pred_features = logreg_features.predict(X_test_feat_scaled)\n",
    "\n",
    "logreg_features_results = {\n",
    "    \"accuracy\":  accuracy_score(y_test, y_pred_features),\n",
    "    \"precision\": precision_score(y_test, y_pred_features),\n",
    "    \"recall\":    recall_score(y_test, y_pred_features),\n",
    "    \"f1_score\":  f1_score(y_test, y_pred_features),\n",
    "    \"train_time_sec\": train_time_features\n",
    "}\n",
    "\n",
    "print(f\"   ‚úì Trained in {train_time_features:.2f}s | Accuracy: {logreg_features_results['accuracy']:.4f} | F1: {logreg_features_results['f1_score']:.4f}\")\n",
    "\n",
    "# ===== COMPARISON =====\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üìä Logistic Regression Comparison\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Metric':<15} {'Pixel-Based':<15} {'Feature-Based':<15} {'Winner':<10}\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Accuracy':<15} {logreg_pixels_results['accuracy']:<15.4f} {logreg_features_results['accuracy']:<15.4f} {'Features' if logreg_features_results['accuracy'] > logreg_pixels_results['accuracy'] else 'Pixels':<10}\")\n",
    "print(f\"{'Precision':<15} {logreg_pixels_results['precision']:<15.4f} {logreg_features_results['precision']:<15.4f} {'Features' if logreg_features_results['precision'] > logreg_pixels_results['precision'] else 'Pixels':<10}\")\n",
    "print(f\"{'Recall':<15} {logreg_pixels_results['recall']:<15.4f} {logreg_features_results['recall']:<15.4f} {'Features' if logreg_features_results['recall'] > logreg_pixels_results['recall'] else 'Pixels':<10}\")\n",
    "print(f\"{'F1-Score':<15} {logreg_pixels_results['f1_score']:<15.4f} {logreg_features_results['f1_score']:<15.4f} {'Features' if logreg_features_results['f1_score'] > logreg_pixels_results['f1_score'] else 'Pixels':<10}\")\n",
    "print(f\"{'Training Time':<15} {logreg_pixels_results['train_time_sec']:<15.2f} {logreg_features_results['train_time_sec']:<15.2f} {'Features' if logreg_features_results['train_time_sec'] < logreg_pixels_results['train_time_sec'] else 'Pixels':<10}\")\n",
    "\n",
    "# Store for later comparison\n",
    "logreg_results = logreg_pixels_results  # Keep pixel-based for backward compatibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd839fd-de3b-4b6f-84df-3d8adb828a6a",
   "metadata": {},
   "source": [
    "### üå≤ Model 2: Random Forest\n",
    "\n",
    "An ensemble of decision trees - let's see how it performs with both approaches!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629c375e-f3a1-417e-a984-c7d9a1f1ff1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import time\n",
    "\n",
    "print(\"\\nüå≤ Training Random Forest with BOTH approaches...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ===== APPROACH 1: Pixel-Based =====\n",
    "print(\"\\n1Ô∏è‚É£ Pixel-Based Approach (784 features)\")\n",
    "start = time.time()\n",
    "rf_pixels = RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1)\n",
    "rf_pixels.fit(X_train, y_train)\n",
    "train_time_pixels = time.time() - start\n",
    "\n",
    "y_pred_pixels = rf_pixels.predict(X_test)\n",
    "\n",
    "rf_pixels_results = {\n",
    "    \"accuracy\":  accuracy_score(y_test, y_pred_pixels),\n",
    "    \"precision\": precision_score(y_test, y_pred_pixels),\n",
    "    \"recall\":    recall_score(y_test, y_pred_pixels),\n",
    "    \"f1_score\":  f1_score(y_test, y_pred_pixels),\n",
    "    \"train_time_sec\": train_time_pixels\n",
    "}\n",
    "\n",
    "print(f\"   ‚úì Trained in {train_time_pixels:.2f}s | Accuracy: {rf_pixels_results['accuracy']:.4f} | F1: {rf_pixels_results['f1_score']:.4f}\")\n",
    "\n",
    "# ===== APPROACH 2: Feature-Based =====\n",
    "print(\"\\n2Ô∏è‚É£ Feature-Based Approach (19 features)\")\n",
    "start = time.time()\n",
    "rf_features = RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1)\n",
    "rf_features.fit(X_train_feat_scaled, y_train)\n",
    "train_time_features = time.time() - start\n",
    "\n",
    "y_pred_features = rf_features.predict(X_test_feat_scaled)\n",
    "\n",
    "rf_features_results = {\n",
    "    \"accuracy\":  accuracy_score(y_test, y_pred_features),\n",
    "    \"precision\": precision_score(y_test, y_pred_features),\n",
    "    \"recall\":    recall_score(y_test, y_pred_features),\n",
    "    \"f1_score\":  f1_score(y_test, y_pred_features),\n",
    "    \"train_time_sec\": train_time_features\n",
    "}\n",
    "\n",
    "print(f\"   ‚úì Trained in {train_time_features:.2f}s | Accuracy: {rf_features_results['accuracy']:.4f} | F1: {rf_features_results['f1_score']:.4f}\")\n",
    "\n",
    "# ===== COMPARISON =====\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üìä Random Forest Comparison\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Metric':<15} {'Pixel-Based':<15} {'Feature-Based':<15} {'Winner':<10}\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Accuracy':<15} {rf_pixels_results['accuracy']:<15.4f} {rf_features_results['accuracy']:<15.4f} {'Features' if rf_features_results['accuracy'] > rf_pixels_results['accuracy'] else 'Pixels':<10}\")\n",
    "print(f\"{'Precision':<15} {rf_pixels_results['precision']:<15.4f} {rf_features_results['precision']:<15.4f} {'Features' if rf_features_results['precision'] > rf_pixels_results['precision'] else 'Pixels':<10}\")\n",
    "print(f\"{'Recall':<15} {rf_pixels_results['recall']:<15.4f} {rf_features_results['recall']:<15.4f} {'Features' if rf_features_results['recall'] > rf_pixels_results['recall'] else 'Pixels':<10}\")\n",
    "print(f\"{'F1-Score':<15} {rf_pixels_results['f1_score']:<15.4f} {rf_features_results['f1_score']:<15.4f} {'Features' if rf_features_results['f1_score'] > rf_pixels_results['f1_score'] else 'Pixels':<10}\")\n",
    "print(f\"{'Training Time':<15} {rf_pixels_results['train_time_sec']:<15.2f} {rf_features_results['train_time_sec']:<15.2f} {'Features' if rf_features_results['train_time_sec'] < rf_pixels_results['train_time_sec'] else 'Pixels':<10}\")\n",
    "\n",
    "# Store for later comparison\n",
    "rf_results = rf_pixels_results  # Keep pixel-based for backward compatibility\n",
    "rf = rf_pixels  # Keep model reference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4d7deb-b74b-4007-a8cd-df9f12a22856",
   "metadata": {},
   "source": [
    "### üéØ Model 3: Support Vector Machine (SVM)\n",
    "\n",
    "A powerful classifier using the RBF kernel - comparing both approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc98fd5d-c05c-4dea-82a9-9ea11436db5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "print(\"\\nüéØ Training SVM with BOTH approaches...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ===== APPROACH 1: Pixel-Based =====\n",
    "print(\"\\n1Ô∏è‚É£ Pixel-Based Approach (784 features)\")\n",
    "start = time.time()\n",
    "svm_pixels = SVC(kernel=\"rbf\", C=5, gamma=\"scale\", random_state=42)\n",
    "svm_pixels.fit(X_train, y_train)\n",
    "train_time_pixels = time.time() - start\n",
    "\n",
    "y_pred_pixels = svm_pixels.predict(X_test)\n",
    "\n",
    "svm_pixels_results = {\n",
    "    \"accuracy\":  accuracy_score(y_test, y_pred_pixels),\n",
    "    \"precision\": precision_score(y_test, y_pred_pixels),\n",
    "    \"recall\":    recall_score(y_test, y_pred_pixels),\n",
    "    \"f1_score\":  f1_score(y_test, y_pred_pixels),\n",
    "    \"train_time_sec\": train_time_pixels\n",
    "}\n",
    "\n",
    "print(f\"   ‚úì Trained in {train_time_pixels:.2f}s | Accuracy: {svm_pixels_results['accuracy']:.4f} | F1: {svm_pixels_results['f1_score']:.4f}\")\n",
    "\n",
    "# ===== APPROACH 2: Feature-Based =====\n",
    "print(\"\\n2Ô∏è‚É£ Feature-Based Approach (19 features)\")\n",
    "start = time.time()\n",
    "svm_features = SVC(kernel=\"rbf\", C=5, gamma=\"scale\", random_state=42)\n",
    "svm_features.fit(X_train_feat_scaled, y_train)\n",
    "train_time_features = time.time() - start\n",
    "\n",
    "y_pred_features = svm_features.predict(X_test_feat_scaled)\n",
    "\n",
    "svm_features_results = {\n",
    "    \"accuracy\":  accuracy_score(y_test, y_pred_features),\n",
    "    \"precision\": precision_score(y_test, y_pred_features),\n",
    "    \"recall\":    recall_score(y_test, y_pred_features),\n",
    "    \"f1_score\":  f1_score(y_test, y_pred_features),\n",
    "    \"train_time_sec\": train_time_features\n",
    "}\n",
    "\n",
    "print(f\"   ‚úì Trained in {train_time_features:.2f}s | Accuracy: {svm_features_results['accuracy']:.4f} | F1: {svm_features_results['f1_score']:.4f}\")\n",
    "\n",
    "# ===== COMPARISON =====\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üìä SVM Comparison\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Metric':<15} {'Pixel-Based':<15} {'Feature-Based':<15} {'Winner':<10}\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Accuracy':<15} {svm_pixels_results['accuracy']:<15.4f} {svm_features_results['accuracy']:<15.4f} {'Features' if svm_features_results['accuracy'] > svm_pixels_results['accuracy'] else 'Pixels':<10}\")\n",
    "print(f\"{'Precision':<15} {svm_pixels_results['precision']:<15.4f} {svm_features_results['precision']:<15.4f} {'Features' if svm_features_results['precision'] > svm_pixels_results['precision'] else 'Pixels':<10}\")\n",
    "print(f\"{'Recall':<15} {svm_pixels_results['recall']:<15.4f} {svm_features_results['recall']:<15.4f} {'Features' if svm_features_results['recall'] > svm_pixels_results['recall'] else 'Pixels':<10}\")\n",
    "print(f\"{'F1-Score':<15} {svm_pixels_results['f1_score']:<15.4f} {svm_features_results['f1_score']:<15.4f} {'Features' if svm_features_results['f1_score'] > svm_pixels_results['f1_score'] else 'Pixels':<10}\")\n",
    "print(f\"{'Training Time':<15} {svm_pixels_results['train_time_sec']:<15.2f} {svm_features_results['train_time_sec']:<15.2f} {'Features' if svm_features_results['train_time_sec'] < svm_pixels_results['train_time_sec'] else 'Pixels':<10}\")\n",
    "\n",
    "# Store for later comparison\n",
    "svm_results = svm_pixels_results  # Keep pixel-based for backward compatibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad57a5b-f52e-4fda-960b-ef614d8b752d",
   "metadata": {},
   "source": [
    "### üë• Model 4: K-Nearest Neighbors (KNN)\n",
    "\n",
    "Instance-based learning - let's see how it handles location-independent features!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0533bd81-9ab2-44e4-8124-33aab6ba2935",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"\\nüîç Finding optimal K for KNN (using feature-based approach)...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "k_values = range(1, 51)\n",
    "accuracies = []\n",
    "\n",
    "for k in k_values:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train_feat_scaled, y_train)\n",
    "    y_pred = knn.predict(X_test_feat_scaled)\n",
    "    accuracies.append(accuracy_score(y_test, y_pred))\n",
    "\n",
    "best_k = k_values[accuracies.index(max(accuracies))]\n",
    "print(f\"\\n‚úÖ Best K: {best_k} with accuracy: {max(accuracies):.4f}\")\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(k_values, accuracies, marker=\"o\", linewidth=2)\n",
    "plt.axvline(x=best_k, color='r', linestyle='--', label=f'Best K={best_k}')\n",
    "plt.xlabel(\"Number of Neighbors (K)\", fontsize=12)\n",
    "plt.ylabel(\"Accuracy\", fontsize=12)\n",
    "plt.title(\"KNN Elbow Criterion - Finding Optimal K\", fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e0e153-039e-4091-afa1-d97a495a5c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "print(f\"\\nüë• Training KNN with k={best_k} using BOTH approaches...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ===== APPROACH 1: Pixel-Based =====\n",
    "print(\"\\n1Ô∏è‚É£ Pixel-Based Approach (784 features)\")\n",
    "start = time.time()\n",
    "knn_pixels = KNeighborsClassifier(n_neighbors=best_k)\n",
    "knn_pixels.fit(X_train, y_train)\n",
    "train_time_pixels = time.time() - start\n",
    "\n",
    "y_pred_pixels = knn_pixels.predict(X_test)\n",
    "\n",
    "knn_pixels_results = {\n",
    "    \"accuracy\":  accuracy_score(y_test, y_pred_pixels),\n",
    "    \"precision\": precision_score(y_test, y_pred_pixels),\n",
    "    \"recall\":    recall_score(y_test, y_pred_pixels),\n",
    "    \"f1_score\":  f1_score(y_test, y_pred_pixels),\n",
    "    \"train_time_sec\": train_time_pixels\n",
    "}\n",
    "\n",
    "print(f\"   ‚úì Trained in {train_time_pixels:.2f}s | Accuracy: {knn_pixels_results['accuracy']:.4f} | F1: {knn_pixels_results['f1_score']:.4f}\")\n",
    "\n",
    "# ===== APPROACH 2: Feature-Based =====\n",
    "print(\"\\n2Ô∏è‚É£ Feature-Based Approach (19 features)\")\n",
    "start = time.time()\n",
    "knn_features = KNeighborsClassifier(n_neighbors=best_k)\n",
    "knn_features.fit(X_train_feat_scaled, y_train)\n",
    "train_time_features = time.time() - start\n",
    "\n",
    "y_pred_features = knn_features.predict(X_test_feat_scaled)\n",
    "\n",
    "knn_features_results = {\n",
    "    \"accuracy\":  accuracy_score(y_test, y_pred_features),\n",
    "    \"precision\": precision_score(y_test, y_pred_features),\n",
    "    \"recall\":    recall_score(y_test, y_pred_features),\n",
    "    \"f1_score\":  f1_score(y_test, y_pred_features),\n",
    "    \"train_time_sec\": train_time_features\n",
    "}\n",
    "\n",
    "print(f\"   ‚úì Trained in {train_time_features:.2f}s | Accuracy: {knn_features_results['accuracy']:.4f} | F1: {knn_features_results['f1_score']:.4f}\")\n",
    "\n",
    "# ===== COMPARISON =====\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üìä KNN Comparison\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Metric':<15} {'Pixel-Based':<15} {'Feature-Based':<15} {'Winner':<10}\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Accuracy':<15} {knn_pixels_results['accuracy']:<15.4f} {knn_features_results['accuracy']:<15.4f} {'Features' if knn_features_results['accuracy'] > knn_pixels_results['accuracy'] else 'Pixels':<10}\")\n",
    "print(f\"{'Precision':<15} {knn_pixels_results['precision']:<15.4f} {knn_features_results['precision']:<15.4f} {'Features' if knn_features_results['precision'] > knn_pixels_results['precision'] else 'Pixels':<10}\")\n",
    "print(f\"{'Recall':<15} {knn_pixels_results['recall']:<15.4f} {knn_features_results['recall']:<15.4f} {'Features' if knn_features_results['recall'] > knn_pixels_results['recall'] else 'Pixels':<10}\")\n",
    "print(f\"{'F1-Score':<15} {knn_pixels_results['f1_score']:<15.4f} {knn_features_results['f1_score']:<15.4f} {'Features' if knn_features_results['f1_score'] > knn_pixels_results['f1_score'] else 'Pixels':<10}\")\n",
    "print(f\"{'Training Time':<15} {knn_pixels_results['train_time_sec']:<15.2f} {knn_features_results['train_time_sec']:<15.2f} {'Features' if knn_features_results['train_time_sec'] < knn_pixels_results['train_time_sec'] else 'Pixels':<10}\")\n",
    "\n",
    "# Store for later comparison\n",
    "knn_results = knn_pixels_results  # Keep pixel-based for backward compatibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a530f4-dc2a-48f0-8354-9c3297ab39d8",
   "metadata": {},
   "source": [
    "### üìä Model 5: Naive Bayes\n",
    "\n",
    "A probabilistic classifier - comparing both approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712f7157-72bf-47be-86c9-0ebb29f261d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "print(\"\\nüìä Training Naive Bayes with BOTH approaches...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ===== APPROACH 1: Pixel-Based =====\n",
    "print(\"\\n1Ô∏è‚É£ Pixel-Based Approach (784 features)\")\n",
    "start = time.time()\n",
    "nb_pixels = GaussianNB()\n",
    "nb_pixels.fit(X_train, y_train)\n",
    "train_time_pixels = time.time() - start\n",
    "\n",
    "y_pred_pixels = nb_pixels.predict(X_test)\n",
    "\n",
    "nb_pixels_results = {\n",
    "    \"accuracy\":  accuracy_score(y_test, y_pred_pixels),\n",
    "    \"precision\": precision_score(y_test, y_pred_pixels),\n",
    "    \"recall\":    recall_score(y_test, y_pred_pixels),\n",
    "    \"f1_score\":  f1_score(y_test, y_pred_pixels),\n",
    "    \"train_time_sec\": train_time_pixels\n",
    "}\n",
    "\n",
    "print(f\"   ‚úì Trained in {train_time_pixels:.2f}s | Accuracy: {nb_pixels_results['accuracy']:.4f} | F1: {nb_pixels_results['f1_score']:.4f}\")\n",
    "\n",
    "# ===== APPROACH 2: Feature-Based =====\n",
    "print(\"\\n2Ô∏è‚É£ Feature-Based Approach (19 features)\")\n",
    "start = time.time()\n",
    "nb_features = GaussianNB()\n",
    "nb_features.fit(X_train_feat_scaled, y_train)\n",
    "train_time_features = time.time() - start\n",
    "\n",
    "y_pred_features = nb_features.predict(X_test_feat_scaled)\n",
    "\n",
    "nb_features_results = {\n",
    "    \"accuracy\":  accuracy_score(y_test, y_pred_features),\n",
    "    \"precision\": precision_score(y_test, y_pred_features),\n",
    "    \"recall\":    recall_score(y_test, y_pred_features),\n",
    "    \"f1_score\":  f1_score(y_test, y_pred_features),\n",
    "    \"train_time_sec\": train_time_features\n",
    "}\n",
    "\n",
    "print(f\"   ‚úì Trained in {train_time_features:.2f}s | Accuracy: {nb_features_results['accuracy']:.4f} | F1: {nb_features_results['f1_score']:.4f}\")\n",
    "\n",
    "# ===== COMPARISON =====\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üìä Naive Bayes Comparison\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Metric':<15} {'Pixel-Based':<15} {'Feature-Based':<15} {'Winner':<10}\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Accuracy':<15} {nb_pixels_results['accuracy']:<15.4f} {nb_features_results['accuracy']:<15.4f} {'Features' if nb_features_results['accuracy'] > nb_pixels_results['accuracy'] else 'Pixels':<10}\")\n",
    "print(f\"{'Precision':<15} {nb_pixels_results['precision']:<15.4f} {nb_features_results['precision']:<15.4f} {'Features' if nb_features_results['precision'] > nb_pixels_results['precision'] else 'Pixels':<10}\")\n",
    "print(f\"{'Recall':<15} {nb_pixels_results['recall']:<15.4f} {nb_features_results['recall']:<15.4f} {'Features' if nb_features_results['recall'] > nb_pixels_results['recall'] else 'Pixels':<10}\")\n",
    "print(f\"{'F1-Score':<15} {nb_pixels_results['f1_score']:<15.4f} {nb_features_results['f1_score']:<15.4f} {'Features' if nb_features_results['f1_score'] > nb_pixels_results['f1_score'] else 'Pixels':<10}\")\n",
    "print(f\"{'Training Time':<15} {nb_pixels_results['train_time_sec']:<15.2f} {nb_features_results['train_time_sec']:<15.2f} {'Features' if nb_features_results['train_time_sec'] < nb_pixels_results['train_time_sec'] else 'Pixels':<10}\")\n",
    "\n",
    "# Store for later comparison\n",
    "nb_results = nb_pixels_results  # Keep pixel-based for backward compatibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cefea6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä Intermediate Summary: Traditional ML Models\n",
    "\n",
    "Let's create a comprehensive comparison of all traditional ML models we've trained!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e411589",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"\\nüìä Comprehensive Model Comparison: Pixel-Based vs Feature-Based\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Collect all results\n",
    "comparison_data = {\n",
    "    'Logistic Regression (Pixels)': logreg_pixels_results,\n",
    "    'Logistic Regression (Features)': logreg_features_results,\n",
    "    'Random Forest (Pixels)': rf_pixels_results,\n",
    "    'Random Forest (Features)': rf_features_results,\n",
    "    'SVM (Pixels)': svm_pixels_results,\n",
    "    'SVM (Features)': svm_features_results,\n",
    "    'KNN (Pixels)': knn_pixels_results,\n",
    "    'KNN (Features)': knn_features_results,\n",
    "    'Naive Bayes (Pixels)': nb_pixels_results,\n",
    "    'Naive Bayes (Features)': nb_features_results,\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "comparison_df = pd.DataFrame(comparison_data).T\n",
    "comparison_df = comparison_df.round(4)\n",
    "\n",
    "# Sort by F1-Score\n",
    "comparison_df_sorted = comparison_df.sort_values('f1_score', ascending=False)\n",
    "\n",
    "print(\"\\n‚úÖ Results compiled!\\n\")\n",
    "display(comparison_df_sorted)\n",
    "\n",
    "# Calculate average improvement\n",
    "pixel_models = ['Logistic Regression (Pixels)', 'Random Forest (Pixels)', 'SVM (Pixels)', 'KNN (Pixels)', 'Naive Bayes (Pixels)']\n",
    "feature_models = ['Logistic Regression (Features)', 'Random Forest (Features)', 'SVM (Features)', 'KNN (Features)', 'Naive Bayes (Features)']\n",
    "\n",
    "avg_f1_pixels = comparison_df.loc[pixel_models, 'f1_score'].mean()\n",
    "avg_f1_features = comparison_df.loc[feature_models, 'f1_score'].mean()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üìà OVERALL COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Average F1-Score (Pixel-Based):   {avg_f1_pixels:.4f}\")\n",
    "print(f\"Average F1-Score (Feature-Based): {avg_f1_features:.4f}\")\n",
    "print(f\"Improvement:                       {((avg_f1_features - avg_f1_pixels) / avg_f1_pixels * 100):+.2f}%\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3fc3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìä Visualizing the comparison...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot 1: F1-Score comparison\n",
    "models = ['LogReg', 'RF', 'SVM', 'KNN', 'NB']\n",
    "pixels_f1 = [logreg_pixels_results['f1_score'], rf_pixels_results['f1_score'], \n",
    "             svm_pixels_results['f1_score'], knn_pixels_results['f1_score'], \n",
    "             nb_pixels_results['f1_score']]\n",
    "features_f1 = [logreg_features_results['f1_score'], rf_features_results['f1_score'], \n",
    "               svm_features_results['f1_score'], knn_features_results['f1_score'], \n",
    "               nb_features_results['f1_score']]\n",
    "\n",
    "x = range(len(models))\n",
    "width = 0.35\n",
    "\n",
    "axes[0].bar([i - width/2 for i in x], pixels_f1, width, label='Pixel-Based', color='#e74c3c', alpha=0.8)\n",
    "axes[0].bar([i + width/2 for i in x], features_f1, width, label='Feature-Based', color='#2ecc71', alpha=0.8)\n",
    "axes[0].set_xlabel('Model', fontsize=12)\n",
    "axes[0].set_ylabel('F1-Score', fontsize=12)\n",
    "axes[0].set_title('F1-Score Comparison: Pixels vs Features', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(models)\n",
    "axes[0].legend()\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Plot 2: Training time comparison\n",
    "pixels_time = [logreg_pixels_results['train_time_sec'], rf_pixels_results['train_time_sec'], \n",
    "               svm_pixels_results['train_time_sec'], knn_pixels_results['train_time_sec'], \n",
    "               nb_pixels_results['train_time_sec']]\n",
    "features_time = [logreg_features_results['train_time_sec'], rf_features_results['train_time_sec'], \n",
    "                 svm_features_results['train_time_sec'], knn_features_results['train_time_sec'], \n",
    "                 nb_features_results['train_time_sec']]\n",
    "\n",
    "axes[1].bar([i - width/2 for i in x], pixels_time, width, label='Pixel-Based', color='#e74c3c', alpha=0.8)\n",
    "axes[1].bar([i + width/2 for i in x], features_time, width, label='Feature-Based', color='#2ecc71', alpha=0.8)\n",
    "axes[1].set_xlabel('Model', fontsize=12)\n",
    "axes[1].set_ylabel('Training Time (seconds)', fontsize=12)\n",
    "axes[1].set_title('Training Time Comparison: Pixels vs Features', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(models)\n",
    "axes[1].legend()\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Visualization complete!\")\n",
    "print(\"\\nüí° Key Takeaway: Feature engineering can significantly improve both performance AND speed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de6d9a0",
   "metadata": {},
   "source": [
    "### üîç Feature Importance Analysis\n",
    "\n",
    "Let's see which features were most important for our Random Forest model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8285f40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüîç Analyzing feature importance from Random Forest (Feature-Based)...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Get feature importances\n",
    "feature_names = X_train_features.columns\n",
    "importances = rf_features.feature_importances_\n",
    "\n",
    "# Create DataFrame and sort\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': importances\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"\\nüìä Top 10 Most Important Features:\")\n",
    "print(\"-\" * 70)\n",
    "for idx, row in importance_df.head(10).iterrows():\n",
    "    print(f\"  {row['Feature']:<25} {row['Importance']:.4f}\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(importance_df['Feature'], importance_df['Importance'], color='#2ecc71')\n",
    "plt.xlabel('Feature Importance', fontsize=12)\n",
    "plt.ylabel('Feature', fontsize=12)\n",
    "plt.title('Feature Importance in Random Forest (Feature-Based)', fontsize=14, fontweight='bold')\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Feature importance analysis complete!\")\n",
    "print(\"\\nüí° This shows which image properties are most discriminative for classification!\")\n",
    "print(\"   Higher importance = more useful for distinguishing benign vs malignant cases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81df6c5-6f5a-4f04-9b06-5241844cd7d9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üß† Deep Learning Approach: Convolutional Neural Network (CNN)\n",
    "\n",
    "Now let's compare our feature engineering approach with deep learning!\n",
    "\n",
    "**Key Difference:** CNNs automatically learn features from raw images through convolutional layers, while our traditional ML models needed handcrafted features.\n",
    "\n",
    "Let's see if the CNN can beat our carefully designed features! üèÜ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8317d1e1-e921-406d-8b8f-f53c554287cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from medmnist import BreastMNIST\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"\\nüñºÔ∏è  Preparing data for CNN...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Load datasets\n",
    "train_ds = BreastMNIST(split='train', download=True)\n",
    "test_ds  = BreastMNIST(split='test',  download=True)\n",
    "\n",
    "# Images as float32 + normalization to [0,1]\n",
    "X_full = train_ds.imgs.astype(\"float32\") / 255.0\n",
    "y_full = train_ds.labels.flatten().astype(\"int32\")\n",
    "\n",
    "X_test_cnn = test_ds.imgs.astype(\"float32\") / 255.0\n",
    "y_test_cnn = test_ds.labels.flatten().astype(\"int32\")\n",
    "\n",
    "# Add channel dimension ‚Üí (N, 28, 28, 1)\n",
    "X_full     = X_full[..., np.newaxis]\n",
    "X_test_cnn = X_test_cnn[..., np.newaxis]\n",
    "\n",
    "print(f\"   Full train shape: {X_full.shape}\")\n",
    "print(f\"   Test shape: {X_test_cnn.shape}\")\n",
    "\n",
    "# Create train/validation split\n",
    "X_train_cnn, X_val_cnn, y_train_cnn, y_val_cnn = train_test_split(\n",
    "    X_full, y_full,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y_full\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Data prepared for CNN\")\n",
    "print(f\"   Training:   {X_train_cnn.shape[0]} samples\")\n",
    "print(f\"   Validation: {X_val_cnn.shape[0]} samples\")\n",
    "print(f\"   Test:       {X_test_cnn.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e66331-2a12-4ca5-9d40-692c4b2db722",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.metrics import AUC\n",
    "\n",
    "print(\"\\nüèóÔ∏è  Building CNN architecture...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1)),\n",
    "    MaxPooling2D((2,2)),\n",
    "    Conv2D(64, (3,3), activation='relu'),\n",
    "    MaxPooling2D((2,2)),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')   # 1 output neuron for binary classification\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', AUC(name='auc')]\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ CNN architecture built!\")\n",
    "print(\"\\nüìê Model Architecture:\")\n",
    "model.summary()\n",
    "\n",
    "# Callbacks\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    \"best_cnn_breastmnist.keras\",\n",
    "    monitor=\"val_loss\",\n",
    "    save_best_only=True,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(\"\\nüí° Using Early Stopping (patience=5) and Model Checkpointing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3b13c6-924b-48ed-8d9d-aee58ed6fefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "print(\"\\nüöÄ Training CNN...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Training + Time measurement\n",
    "start = time.time()\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_cnn, y_train_cnn,\n",
    "    validation_data=(X_val_cnn, y_val_cnn),\n",
    "    epochs=30,\n",
    "    batch_size=64,\n",
    "    callbacks=[early_stop, checkpoint],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "train_time = time.time() - start\n",
    "\n",
    "# Evaluation\n",
    "y_prob = model.predict(X_test_cnn, verbose=0)\n",
    "y_pred = (y_prob > 0.5).astype(int).flatten()\n",
    "\n",
    "cnn_results = {\n",
    "    \"accuracy\":        accuracy_score(y_test_cnn, y_pred),\n",
    "    \"precision\":       precision_score(y_test_cnn, y_pred),\n",
    "    \"recall\":          recall_score(y_test_cnn, y_pred),\n",
    "    \"f1_score\":        f1_score(y_test_cnn, y_pred),\n",
    "    \"train_time_sec\":  train_time\n",
    "}\n",
    "\n",
    "print(f\"\\n‚úÖ CNN trained in {train_time:.2f} seconds\")\n",
    "print(f\"\\nüìä Performance Metrics:\")\n",
    "print(f\"   Accuracy:  {cnn_results['accuracy']:.4f}\")\n",
    "print(f\"   Precision: {cnn_results['precision']:.4f}\")\n",
    "print(f\"   Recall:    {cnn_results['recall']:.4f}\")\n",
    "print(f\"   F1-Score:  {cnn_results['f1_score']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543a32bd-d101-438e-9aef-f27a6ac8ce70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"\\nüìà Visualizing training history...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(history.history['accuracy'], label='Train Acc', linewidth=2)\n",
    "plt.plot(history.history['val_accuracy'], label='Val Acc', linewidth=2)\n",
    "plt.title(\"Accuracy over Epochs\", fontsize=12, fontweight='bold')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(history.history['loss'], label='Train Loss', linewidth=2)\n",
    "plt.plot(history.history['val_loss'], label='Val Loss', linewidth=2)\n",
    "plt.title(\"Loss over Epochs\", fontsize=12, fontweight='bold')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(history.history['auc'], label='Train AUC', linewidth=2)\n",
    "plt.plot(history.history['val_auc'], label='Val AUC', linewidth=2)\n",
    "plt.title(\"AUC over Epochs\", fontsize=12, fontweight='bold')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"AUC\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Training visualization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bc62ce-b300-4b9d-8486-6eb22aa377b2",
   "metadata": {},
   "source": [
    "## üìä STEP 3: Final Model Comparison & Results\n",
    "\n",
    "Let's put everything together and see which approach works best!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f437c2f5-9159-4953-b4de-69e37933ba5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print(\"\\nüìä Final Comparison: All Approaches\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Collect best results from each approach\n",
    "final_results = {\n",
    "    \"LogReg (Pixels)\": logreg_pixels_results,\n",
    "    \"LogReg (Features)\": logreg_features_results,\n",
    "    \"Random Forest (Pixels)\": rf_pixels_results,\n",
    "    \"Random Forest (Features)\": rf_features_results,\n",
    "    \"SVM (Pixels)\": svm_pixels_results,\n",
    "    \"SVM (Features)\": svm_features_results,\n",
    "    \"KNN (Pixels)\": knn_pixels_results,\n",
    "    \"KNN (Features)\": knn_features_results,\n",
    "    \"Naive Bayes (Pixels)\": nb_pixels_results,\n",
    "    \"Naive Bayes (Features)\": nb_features_results,\n",
    "    \"CNN (Deep Learning)\": cnn_results\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "results_df = pd.DataFrame(final_results).T\n",
    "results_df = results_df.round(4)\n",
    "\n",
    "# Sort by F1-Score\n",
    "results_df = results_df.sort_values('f1_score', ascending=False)\n",
    "\n",
    "print(\"\\n‚úÖ Results compiled!\\n\")\n",
    "print(\"=\" * 70)\n",
    "print(\"üìà COMPLETE MODEL COMPARISON TABLE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "display(results_df)\n",
    "\n",
    "# Find best model\n",
    "best_model = results_df.index[0]\n",
    "best_f1 = results_df.loc[best_model, 'f1_score']\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(f\"üèÜ BEST MODEL: {best_model}\")\n",
    "print(f\"   F1-Score: {best_f1:.4f}\")\n",
    "print(f\"   Accuracy: {results_df.loc[best_model, 'accuracy']:.4f}\")\n",
    "print(f\"   Training Time: {results_df.loc[best_model, 'train_time_sec']:.2f} seconds\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Summary statistics by approach\n",
    "print(\"\\nüìä Summary by Approach:\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "pixel_based = results_df[[('Pixels' in idx or 'CNN' in idx) for idx in results_df.index]]\n",
    "feature_based = results_df[['Features' in idx for idx in results_df.index]]\n",
    "\n",
    "print(f\"\\n  Pixel-Based Models (avg):  F1={pixel_based['f1_score'].mean():.4f}\")\n",
    "print(f\"  Feature-Based Models (avg): F1={feature_based['f1_score'].mean():.4f}\")\n",
    "print(f\"  CNN (Deep Learning):        F1={cnn_results['f1_score']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af52f6d-0b1d-431a-9a24-c6d7b669628e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"\\nüìä Visualizing complete model comparison...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Plot 1: F1-Score comparison (grouped by model type)\n",
    "axes[0, 0].barh(range(len(results_df)), results_df['f1_score'], \n",
    "                color=['#3498db' if 'Pixels' in idx else '#2ecc71' if 'Features' in idx else '#9b59b6' \n",
    "                       for idx in results_df.index])\n",
    "axes[0, 0].set_yticks(range(len(results_df)))\n",
    "axes[0, 0].set_yticklabels(results_df.index, fontsize=9)\n",
    "axes[0, 0].set_xlabel('F1-Score', fontsize=11)\n",
    "axes[0, 0].set_title('F1-Score Comparison (All Models)', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Add legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [\n",
    "    Patch(facecolor='#3498db', label='Pixel-Based'),\n",
    "    Patch(facecolor='#2ecc71', label='Feature-Based'),\n",
    "    Patch(facecolor='#9b59b6', label='CNN (Deep Learning)')\n",
    "]\n",
    "axes[0, 0].legend(handles=legend_elements, loc='lower right')\n",
    "\n",
    "# Plot 2: Accuracy comparison\n",
    "axes[0, 1].barh(range(len(results_df)), results_df['accuracy'], \n",
    "                color=['#3498db' if 'Pixels' in idx else '#2ecc71' if 'Features' in idx else '#9b59b6' \n",
    "                       for idx in results_df.index])\n",
    "axes[0, 1].set_yticks(range(len(results_df)))\n",
    "axes[0, 1].set_yticklabels(results_df.index, fontsize=9)\n",
    "axes[0, 1].set_xlabel('Accuracy', fontsize=11)\n",
    "axes[0, 1].set_title('Accuracy Comparison (All Models)', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Plot 3: Training time comparison\n",
    "axes[1, 0].barh(range(len(results_df)), results_df['train_time_sec'], \n",
    "                color=['#3498db' if 'Pixels' in idx else '#2ecc71' if 'Features' in idx else '#9b59b6' \n",
    "                       for idx in results_df.index])\n",
    "axes[1, 0].set_yticks(range(len(results_df)))\n",
    "axes[1, 0].set_yticklabels(results_df.index, fontsize=9)\n",
    "axes[1, 0].set_xlabel('Training Time (seconds)', fontsize=11)\n",
    "axes[1, 0].set_title('Training Time Comparison', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Plot 4: Precision vs Recall scatter\n",
    "for idx in results_df.index:\n",
    "    if 'Pixels' in idx:\n",
    "        color = '#3498db'\n",
    "    elif 'Features' in idx:\n",
    "        color = '#2ecc71'\n",
    "    else:\n",
    "        color = '#9b59b6'\n",
    "    \n",
    "    axes[1, 1].scatter(results_df.loc[idx, 'recall'], \n",
    "                      results_df.loc[idx, 'precision'], \n",
    "                      s=150, alpha=0.7, color=color)\n",
    "    \n",
    "axes[1, 1].set_xlabel('Recall', fontsize=11)\n",
    "axes[1, 1].set_ylabel('Precision', fontsize=11)\n",
    "axes[1, 1].set_title('Precision vs Recall Trade-off', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].grid(alpha=0.3)\n",
    "axes[1, 1].legend(handles=legend_elements)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Visualization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562211b8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéì Summary & Key Takeaways\n",
    "\n",
    "### üìù What We Learned\n",
    "\n",
    "#### 1. **The Location Problem in Medical Imaging**\n",
    "   - Raw pixel values are **location-dependent** - tumors at different positions create completely different pixel patterns\n",
    "   - This makes learning difficult for traditional ML models\n",
    "\n",
    "#### 2. **Feature Engineering Solution**\n",
    "   - We extracted **19 location-independent features** (contrast, entropy, edge strength, etc.)\n",
    "   - These features describe WHAT is in the image, not WHERE it is\n",
    "   - Result: Often **better performance** with **41x fewer features** (19 vs 784)\n",
    "\n",
    "#### 3. **Approach Comparison**\n",
    "\n",
    "| Approach | # Features | Pros | Cons |\n",
    "|----------|-----------|------|------|\n",
    "| **Pixel-Based** | 784 | Simple, no preprocessing | Location-dependent, high dimensionality |\n",
    "| **Feature-Based** | 19 | Location-independent, interpretable, fast | Requires domain knowledge |\n",
    "| **CNN (Deep Learning)** | Learned | Automatic feature learning, powerful | Needs more data, slower training, black box |\n",
    "\n",
    "### üîë Key Insights\n",
    "\n",
    "#### Performance Insights:\n",
    "- **Feature engineering matters!** Handcrafted features often matched or beat pixel-based approaches\n",
    "- **CNNs excel at image tasks** because they automatically learn spatial features through convolutions\n",
    "- **Trade-offs exist**: Traditional ML with good features trains faster; CNNs potentially perform better with more data\n",
    "\n",
    "#### Medical Context:\n",
    "- **High recall is critical** in cancer detection - we want to catch all malignant cases\n",
    "- **Interpretable features** (contrast, texture) help clinicians understand the model's reasoning\n",
    "- **Fast inference** matters in clinical settings - feature-based models predict instantly\n",
    "\n",
    "### üí° Practical Lessons\n",
    "\n",
    "1. **Start with feature engineering** - understand your problem domain\n",
    "2. **Compare approaches** - don't assume deep learning is always best\n",
    "3. **Consider constraints** - training time, interpretability, data availability\n",
    "4. **Medical AI requires** - high reliability, interpretability, and validation\n",
    "\n",
    "### üöÄ Next Steps\n",
    "\n",
    "To further improve:\n",
    "- **Hyperparameter tuning** (GridSearchCV) for all models\n",
    "- **More sophisticated features** (texture patterns, shape descriptors)\n",
    "- **Data augmentation** for CNN (rotation, flipping, brightness adjustment)\n",
    "- **Ensemble methods** combining multiple models\n",
    "- **Transfer learning** using pre-trained medical imaging networks\n",
    "- **Cross-validation** for more robust performance estimates\n",
    "\n",
    "---\n",
    "\n",
    "**üéâ Congratulations! You've completed a comprehensive medical image classification workshop!**\n",
    "\n",
    "**Key Message:** Feature engineering isn't dead - it's a powerful tool that can match or complement deep learning, especially when you understand your domain and have limited data!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uv-notebooks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
